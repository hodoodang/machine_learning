{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경사하강법 : Gradient Descent Optimization  \n",
    "경사 : 미분하면 경사를 구할 수 있다.(기울기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=19, shape=(5,), dtype=float32, numpy=array([1., 2., 3., 4., 5.], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1,2,3,4,5.0] # 실수로 만들기 위해 .0을 하나 붙입니다.\n",
    "y = [3,5,7,9,11.0]\n",
    "# y = 2 * x + 1 -> y = W * x + b\n",
    "W = tf.Variable(1.0) # 변수 : 학습되는 파라미터, 초기값 1.0\n",
    "b = tf.Variable(0.0) # 초기값 0.0\n",
    "\n",
    "h = W * x + b # Hyperthesis 가설, 예측\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x$와 $y$가 주어졌을 때 $ h = W \\times X + b $의 계산식을 완성하는 $W$와 $b$의 값을 찾아보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=24, shape=(), dtype=float32, numpy=18.0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 오차 측정\n",
    "cost = tf.reduce_mean(tf.square(h - y)) # Mean 평균, Square 제곱 Error 오차\n",
    "cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W와 b값의 변화를 주어 cost(오차)를 줄인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=48, shape=(), dtype=float32, numpy=0.0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = tf.Variable(2.0)\n",
    "b = tf.Variable(1.0)\n",
    "h = W * x + b\n",
    "cost = tf.reduce_mean(tf.square(h - y))\n",
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: id=171, shape=(), dtype=float32, numpy=60.0>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = tf.Variable(5.0)\n",
    "b = tf.Variable(0.0)\n",
    "with tf.GradientTape() as tape: # 경사 기록 장치: 이 안에서 수행되는 연산의 경사가 기록됩니다.\n",
    "    h = W * x + b\n",
    "    cost = tf.reduce_mean(tf.square(h-y))\n",
    "w_grad = tape.gradient(cost, [W])\n",
    "w_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W의 초기값을 1.0으로 잡게 되면 경사값은 -28.0  \n",
    "W의 초기값을 5.0으로 잡게 되면 경사값은 60.0  \n",
    "이 나오게 되는데 이 말은 $W$의 값을 경사값만큼 빼라는 의미이다.  \n",
    "**그러나** 이 값을 그대로 빼게 되면 너무도 변화량이 커지며 값도 커지기 때문에 학습률이라는 것을 곱한 결과값을 빼야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(10.0)\n",
    "b = tf.Variable(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.2727277>,\n",
       " [<tf.Tensor: id=3648, shape=(), dtype=float32, numpy=1.0251999e-05>])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.GradientTape() as tape: # 경사 기록 장치: 이 안에서 수행되는 연산의 경사가 기록됩니다.\n",
    "    h = W * x + b\n",
    "    cost = tf.reduce_mean(tf.square(h-y))\n",
    "w_grad = tape.gradient(cost, [W])\n",
    "learning_rate = 0.01\n",
    "W.assign_sub(learning_rate * w_grad[0])\n",
    "W, w_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 코드를 반복하는 과정을 통해 초기값이 5.0이었던 $W$의 값이 정답인 2.0에 가까워지는 것을 발견할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.027979>,\n",
       " <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.8989878>)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    with tf.GradientTape() as tape: # 경사 기록 장치: 이 안에서 수행되는 연산의 경사가 기록됩니다.\n",
    "        h = W * x + b\n",
    "        cost = tf.reduce_mean(tf.square(h-y))\n",
    "    w_grad, b_grad = tape.gradient(cost, [W, b])\n",
    "    learning_rate = 0.01\n",
    "    W.assign_sub(learning_rate * w_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1000번 반복한 결과 $W$는 2에 $b$는 1에 가까워진 것을 알 수 있다.(초기값: (10, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,2,3,4,5.0]\n",
    "y = [3,5,7,9,11.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model = keras.Sequencial()`의 경우 기본적, 기초적인 사용법이라 커스터마이징이 어렵습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "class SimpleModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        print(\"init\")\n",
    "        self.dense_0 = keras.layers.Dense(1) # dense: 완전연결\n",
    "        # layer를 추가하고 싶은 경우 바로 위의 코드와 같이 이곳에 작성\n",
    "        \n",
    "    def call(self, x):\n",
    "        h = self.dense_0(x)\n",
    "        h = tf.squeeze(h, axis=1) # (m,1)을 (m)으로 차원축소합니다.\n",
    "        return h\n",
    "    \n",
    "model = SimpleModel()\n",
    "optimizer = tf.optimizers.SGD(learning_rate=0.07)\n",
    "\n",
    "def loss(y, h):\n",
    "    return tf.reduce_mean(tf.square(y-h))\n",
    "def update(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        h = model(x)\n",
    "        cost = loss(y, h)\n",
    "    grads = tape.gradient(cost, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish [<tf.Variable 'simple_model_13/dense_11/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[2.0009394]], dtype=float32)>, <tf.Variable 'simple_model_13/dense_11/bias:0' shape=(1,) dtype=float32, numpy=array([0.99660903], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "x_2d = np.array(x).reshape([-1,1]).astype(np.float32)\n",
    "y = np.array(y).astype(np.float32)\n",
    "for i in range(200):\n",
    "    update(x_2d, y)\n",
    "print('finish', model.trainable_variables) # 값 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "약 200번 반복한 결과 $W$는 2.0009394로 2에 아주 가까운 수가 $b$는 0.99660903으로 1에 아주 가까운 수가 나오는 것을 확인할 수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
