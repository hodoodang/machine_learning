{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 4), (150,), {0, 1, 2})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = load_iris(return_X_y=True)\n",
    "x.shape, y.shape, set(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "총 150개의 데이터가 있으며, {0, 1, 2}라는 3개의 클래스가 있음을 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(keras.Model):\n",
    "    \n",
    "    '''\n",
    "    __init__ 생성자 함수\n",
    "    optimizer: SGD(확률적 경사하강법) learning_rate = 0.01\n",
    "    하나의 dense 층을 사용\n",
    "    3개의 클래스이므로 units=3\n",
    "    activation function: softmax\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__() # 상속한 클래스의 생성자 호출\n",
    "        self.opt = tf.optimizers.SGD(learning_rate=0.01)\n",
    "        self.dense = keras.layers.Dense(units=3, activation=keras.activations.softmax)\n",
    "    \n",
    "    '''\n",
    "    입력값을 받아\n",
    "    생성자에서 만든 layer를 불러와 학습을 진행한다.\n",
    "    '''\n",
    "    def call(self, x):\n",
    "        h = self.dense(x)\n",
    "        return h\n",
    "    \n",
    "    def get_loss(self, y, h):\n",
    "        cross_entropy = - (y * tf.math.log(h) + (1-y) * tf.math.log(1-h))\n",
    "        loss = tf.reduce_mean(cross_entropy)\n",
    "        return loss\n",
    "    \n",
    "    '''\n",
    "    정확도 측정 함수\n",
    "    h 3개의 확률: (m,3), y: (m)\n",
    "    '''\n",
    "    def get_accuracy(self, y, h):\n",
    "        predict = tf.argmax(h, -1)\n",
    "        self.acc = tf.reduce_mean(tf.cast(tf.equal(y, predict), tf.float32)) \n",
    "        # True > 1, False > 0\n",
    "    \n",
    "    '''\n",
    "    1차원이던 y를 one hot 인코딩을 통해 (m,3) 차원으로 바꿔준다.\n",
    "    epoch 만큼 경사하강을 진행한다. \n",
    "    경사하강을 진행하는 동안 경사 기록 장치를 통해 변경되는 값들을 기록한다.\n",
    "    또한 경사 값을 계산하고 가중치에서 경사를 빼는 과정을 수행한다.\n",
    "    \n",
    "    x : (m, 4), y : (m)\n",
    "    '''\n",
    "    def train(self, x, y, epoch=1):\n",
    "        y_hot = tf.one_hot(y, depth=3, axis=-1)\n",
    "        y_hot = tf.cast(y_hot, tf.float32)\n",
    "        for i in range(epoch):\n",
    "            with tf.GradientTape() as tape:\n",
    "                h = self.call(x)\n",
    "                loss = self.get_loss(y_hot, h)\n",
    "            grads = tape.gradient(loss, self.trainable_variables) # 경사 계산\n",
    "            self.opt.apply_gradients(zip(grads, self.trainable_variables)) # 가중치에서 경사 빼기\n",
    "            self.get_accuracy(y, h)\n",
    "            print(f'{i}/{epoch} loss: {loss} acc: {self.acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dense_8 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "0/100 loss: 1.311767816543579 acc: 0.3333333432674408\n",
      "1/100 loss: 1.2121511697769165 acc: 0.3333333432674408\n",
      "2/100 loss: 1.120406985282898 acc: 0.3333333432674408\n",
      "3/100 loss: 1.0366894006729126 acc: 0.3333333432674408\n",
      "4/100 loss: 0.9609566926956177 acc: 0.3333333432674408\n",
      "5/100 loss: 0.8930127620697021 acc: 0.3400000035762787\n",
      "6/100 loss: 0.8325497508049011 acc: 0.35333332419395447\n",
      "7/100 loss: 0.7791854739189148 acc: 0.36000001430511475\n",
      "8/100 loss: 0.7324867844581604 acc: 0.36666667461395264\n",
      "9/100 loss: 0.691979706287384 acc: 0.36666667461395264\n",
      "10/100 loss: 0.6571547389030457 acc: 0.36666667461395264\n",
      "11/100 loss: 0.6274710893630981 acc: 0.3866666555404663\n",
      "12/100 loss: 0.6023666858673096 acc: 0.3933333456516266\n",
      "13/100 loss: 0.5812740921974182 acc: 0.4000000059604645\n",
      "14/100 loss: 0.5636388063430786 acc: 0.4000000059604645\n",
      "15/100 loss: 0.5489382147789001 acc: 0.40666666626930237\n",
      "16/100 loss: 0.5366960763931274 acc: 0.40666666626930237\n",
      "17/100 loss: 0.5264909267425537 acc: 0.41333332657814026\n",
      "18/100 loss: 0.5179601907730103 acc: 0.41333332657814026\n",
      "19/100 loss: 0.5107976794242859 acc: 0.4333333373069763\n",
      "20/100 loss: 0.5047500729560852 acc: 0.4399999976158142\n",
      "21/100 loss: 0.49960947036743164 acc: 0.4533333480358124\n",
      "22/100 loss: 0.49520695209503174 acc: 0.47333332896232605\n",
      "23/100 loss: 0.49140626192092896 acc: 0.47999998927116394\n",
      "24/100 loss: 0.48809733986854553 acc: 0.47999998927116394\n",
      "25/100 loss: 0.4851909577846527 acc: 0.4866666793823242\n",
      "26/100 loss: 0.48261559009552 acc: 0.5133333206176758\n",
      "27/100 loss: 0.48031318187713623 acc: 0.5333333611488342\n",
      "28/100 loss: 0.47823622822761536 acc: 0.5533333420753479\n",
      "29/100 loss: 0.476346492767334 acc: 0.5600000023841858\n",
      "30/100 loss: 0.4746125638484955 acc: 0.5733333230018616\n",
      "31/100 loss: 0.47300827503204346 acc: 0.5733333230018616\n",
      "32/100 loss: 0.4715127944946289 acc: 0.6266666650772095\n",
      "33/100 loss: 0.47010836005210876 acc: 0.6399999856948853\n",
      "34/100 loss: 0.46878060698509216 acc: 0.6600000262260437\n",
      "35/100 loss: 0.46751755475997925 acc: 0.6600000262260437\n",
      "36/100 loss: 0.4663093686103821 acc: 0.6666666865348816\n",
      "37/100 loss: 0.4651479125022888 acc: 0.6666666865348816\n",
      "38/100 loss: 0.46402615308761597 acc: 0.6666666865348816\n",
      "39/100 loss: 0.46293875575065613 acc: 0.6666666865348816\n",
      "40/100 loss: 0.4618805944919586 acc: 0.6666666865348816\n",
      "41/100 loss: 0.4608481526374817 acc: 0.6666666865348816\n",
      "42/100 loss: 0.45983803272247314 acc: 0.6666666865348816\n",
      "43/100 loss: 0.4588475525379181 acc: 0.6800000071525574\n",
      "44/100 loss: 0.45787447690963745 acc: 0.6933333277702332\n",
      "45/100 loss: 0.45691683888435364 acc: 0.699999988079071\n",
      "46/100 loss: 0.4559732675552368 acc: 0.7066666483879089\n",
      "47/100 loss: 0.45504239201545715 acc: 0.7066666483879089\n",
      "48/100 loss: 0.45412296056747437 acc: 0.7066666483879089\n",
      "49/100 loss: 0.453214168548584 acc: 0.7133333086967468\n",
      "50/100 loss: 0.4523153603076935 acc: 0.7200000286102295\n",
      "51/100 loss: 0.4514256715774536 acc: 0.7200000286102295\n",
      "52/100 loss: 0.4505447745323181 acc: 0.7200000286102295\n",
      "53/100 loss: 0.4496719241142273 acc: 0.7200000286102295\n",
      "54/100 loss: 0.44880715012550354 acc: 0.7200000286102295\n",
      "55/100 loss: 0.44794976711273193 acc: 0.7266666889190674\n",
      "56/100 loss: 0.4470995366573334 acc: 0.7266666889190674\n",
      "57/100 loss: 0.4462563693523407 acc: 0.7266666889190674\n",
      "58/100 loss: 0.44541996717453003 acc: 0.7400000095367432\n",
      "59/100 loss: 0.4445902109146118 acc: 0.7400000095367432\n",
      "60/100 loss: 0.443766713142395 acc: 0.746666669845581\n",
      "61/100 loss: 0.44294968247413635 acc: 0.746666669845581\n",
      "62/100 loss: 0.44213858246803284 acc: 0.746666669845581\n",
      "63/100 loss: 0.4413335621356964 acc: 0.746666669845581\n",
      "64/100 loss: 0.4405345618724823 acc: 0.746666669845581\n",
      "65/100 loss: 0.4397413730621338 acc: 0.746666669845581\n",
      "66/100 loss: 0.43895384669303894 acc: 0.746666669845581\n",
      "67/100 loss: 0.4381721317768097 acc: 0.746666669845581\n",
      "68/100 loss: 0.43739598989486694 acc: 0.746666669845581\n",
      "69/100 loss: 0.43662524223327637 acc: 0.753333330154419\n",
      "70/100 loss: 0.4358600974082947 acc: 0.753333330154419\n",
      "71/100 loss: 0.4351002871990204 acc: 0.753333330154419\n",
      "72/100 loss: 0.43434587121009827 acc: 0.753333330154419\n",
      "73/100 loss: 0.433596670627594 acc: 0.753333330154419\n",
      "74/100 loss: 0.4328526258468628 acc: 0.753333330154419\n",
      "75/100 loss: 0.4321138262748718 acc: 0.753333330154419\n",
      "76/100 loss: 0.4313802123069763 acc: 0.753333330154419\n",
      "77/100 loss: 0.4306516647338867 acc: 0.753333330154419\n",
      "78/100 loss: 0.42992812395095825 acc: 0.753333330154419\n",
      "79/100 loss: 0.42920947074890137 acc: 0.753333330154419\n",
      "80/100 loss: 0.4284959137439728 acc: 0.753333330154419\n",
      "81/100 loss: 0.42778709530830383 acc: 0.753333330154419\n",
      "82/100 loss: 0.42708316445350647 acc: 0.753333330154419\n",
      "83/100 loss: 0.4263840317726135 acc: 0.753333330154419\n",
      "84/100 loss: 0.42568960785865784 acc: 0.753333330154419\n",
      "85/100 loss: 0.4249998927116394 acc: 0.7599999904632568\n",
      "86/100 loss: 0.4243148863315582 acc: 0.7666666507720947\n",
      "87/100 loss: 0.42363443970680237 acc: 0.7666666507720947\n",
      "88/100 loss: 0.422958642244339 acc: 0.7666666507720947\n",
      "89/100 loss: 0.42228731513023376 acc: 0.7733333110809326\n",
      "90/100 loss: 0.42162051796913147 acc: 0.7733333110809326\n",
      "91/100 loss: 0.4209582209587097 acc: 0.7733333110809326\n",
      "92/100 loss: 0.4203002154827118 acc: 0.7733333110809326\n",
      "93/100 loss: 0.41964662075042725 acc: 0.7733333110809326\n",
      "94/100 loss: 0.4189974367618561 acc: 0.7733333110809326\n",
      "95/100 loss: 0.41835248470306396 acc: 0.7733333110809326\n",
      "96/100 loss: 0.4177117645740509 acc: 0.7733333110809326\n",
      "97/100 loss: 0.417075514793396 acc: 0.7733333110809326\n",
      "98/100 loss: 0.4164431691169739 acc: 0.7733333110809326\n",
      "99/100 loss: 0.4158150255680084 acc: 0.7733333110809326\n"
     ]
    }
   ],
   "source": [
    "model.train(x, y, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10에서 진행한 digit을 keras로 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10개의 클래스가 존재 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1797, 64), (1797,), {0, 1, 2, 3, 4, 5, 6, 7, 8, 9})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = load_digits(return_X_y=True)\n",
    "x.shape, y.shape, set(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련 데이터 / 테스트 데이터 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = len(x) // 2\n",
    "x_train, x_test = x[:split_index], x[split_index:]\n",
    "y_train, y_test = y[:split_index], y[split_index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "간단한 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model = keras.Sequential()\n",
    "simple_model.add(keras.layers.Flatten(input_shape=(8*8,)))\n",
    "simple_model.add(keras.layers.Dense(128, activation='relu'))\n",
    "simple_model.add(keras.layers.Dense(128, activation='relu'))\n",
    "simple_model.add(keras.layers.Dense(10, activation=keras.layers.Softmax()))\n",
    "\n",
    "simple_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',\n",
    "                    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_6 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 26,122\n",
      "Trainable params: 26,122\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "simple_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 898 samples\n",
      "Epoch 1/7\n",
      "898/898 [==============================] - 0s 324us/sample - loss: 1.5744 - accuracy: 0.5612\n",
      "Epoch 2/7\n",
      "898/898 [==============================] - 0s 41us/sample - loss: 0.2296 - accuracy: 0.9354\n",
      "Epoch 3/7\n",
      "898/898 [==============================] - 0s 42us/sample - loss: 0.0970 - accuracy: 0.9800\n",
      "Epoch 4/7\n",
      "898/898 [==============================] - 0s 40us/sample - loss: 0.0605 - accuracy: 0.9889\n",
      "Epoch 5/7\n",
      "898/898 [==============================] - 0s 41us/sample - loss: 0.0359 - accuracy: 0.9955\n",
      "Epoch 6/7\n",
      "898/898 [==============================] - 0s 40us/sample - loss: 0.0272 - accuracy: 0.9967\n",
      "Epoch 7/7\n",
      "898/898 [==============================] - 0s 41us/sample - loss: 0.0165 - accuracy: 1.0000\n",
      "899/1 - 0s - loss: 0.1740 - accuracy: 0.9088\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.32392970074146354, 0.90878755]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_model.fit(x_train, y_train, epochs=7)\n",
    "\n",
    "simple_model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.opt = tf.optimizers.Adam(learning_rate=0.0002)\n",
    "        self.conv1 = keras.layers.Conv2D(64, 3, padding='same', input_shape=(8,8,1), activation='relu')\n",
    "        self.max1 = keras.layers.MaxPool2D((2,2), padding='same')\n",
    "        self.conv2 = keras.layers.Conv2D(128, 3, input_shape=(8,8,1), activation='relu')\n",
    "        self.max2 = keras.layers.MaxPool2D((2,2), padding='same')\n",
    "        self.flatten = keras.layers.Flatten()\n",
    "        self.d1 = keras.layers.Dense(128, activation='relu')\n",
    "        self.d2 = keras.layers.Dense(10, activation='softmax')\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "#         x = self.max1(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.max2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.d1(x)\n",
    "        return self.d2(x)\n",
    "    \n",
    "    def get_loss(self, y, h):\n",
    "        cross_entropy = - (y * tf.math.log(h) + (1-y) * tf.math.log(1-h))\n",
    "        loss = tf.reduce_mean(cross_entropy)\n",
    "        return loss\n",
    "    \n",
    "    def get_accuracy(self, y, h):\n",
    "        predict = tf.argmax(h, -1)\n",
    "        self.acc = tf.reduce_mean(tf.cast(tf.equal(y, predict), tf.float32))\n",
    "        \n",
    "    def train(self, x, y, epoch=1):\n",
    "        y_hot = tf.one_hot(y, depth=10, axis=-1)\n",
    "        y_hot = tf.cast(y_hot, tf.float32)\n",
    "        for i in range(epoch):\n",
    "            with tf.GradientTape() as tape:\n",
    "                h = self.call(x)\n",
    "                loss = self.get_loss(y_hot, h)\n",
    "            grads = tape.gradient(loss, self.trainable_variables) # 경사 계산\n",
    "            self.opt.apply_gradients(zip(grads, self.trainable_variables)) # 가중치에서 경사 빼기\n",
    "            self.get_accuracy(y, h)\n",
    "            print(f'{i}/{epoch} loss: {loss} acc: {self.acc}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = x_train.reshape((-1,8,8,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer conv2d_27 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "0/100 loss: 0.3816850781440735 acc: 0.15033407509326935\n",
      "1/100 loss: 0.32019829750061035 acc: 0.2327394187450409\n",
      "2/100 loss: 0.278273344039917 acc: 0.390868604183197\n",
      "3/100 loss: 0.24241940677165985 acc: 0.5534521341323853\n",
      "4/100 loss: 0.20956459641456604 acc: 0.685968816280365\n",
      "5/100 loss: 0.17866134643554688 acc: 0.7616926431655884\n",
      "6/100 loss: 0.15194232761859894 acc: 0.8151447772979736\n",
      "7/100 loss: 0.12899085879325867 acc: 0.858574628829956\n",
      "8/100 loss: 0.11048834025859833 acc: 0.8763920068740845\n",
      "9/100 loss: 0.09675676375627518 acc: 0.8875278234481812\n",
      "10/100 loss: 0.08608819544315338 acc: 0.8919821977615356\n",
      "11/100 loss: 0.07633233070373535 acc: 0.904231607913971\n",
      "12/100 loss: 0.06708924472332001 acc: 0.9120267033576965\n",
      "13/100 loss: 0.05910087749361992 acc: 0.9276169538497925\n",
      "14/100 loss: 0.052873097360134125 acc: 0.9354120492935181\n",
      "15/100 loss: 0.048086389899253845 acc: 0.9376391768455505\n",
      "16/100 loss: 0.044121742248535156 acc: 0.9409799575805664\n",
      "17/100 loss: 0.040516406297683716 acc: 0.9443207383155823\n",
      "18/100 loss: 0.03715613856911659 acc: 0.948775053024292\n",
      "19/100 loss: 0.03413303196430206 acc: 0.9543429613113403\n",
      "20/100 loss: 0.03147568553686142 acc: 0.9599109292030334\n",
      "21/100 loss: 0.029146257787942886 acc: 0.9632516503334045\n",
      "22/100 loss: 0.027069393545389175 acc: 0.967706024646759\n",
      "23/100 loss: 0.025203784927725792 acc: 0.9710467457771301\n",
      "24/100 loss: 0.023550033569335938 acc: 0.9732739329338074\n",
      "25/100 loss: 0.022064993157982826 acc: 0.974387526512146\n",
      "26/100 loss: 0.020705875009298325 acc: 0.9777283072471619\n",
      "27/100 loss: 0.019434211775660515 acc: 0.9777283072471619\n",
      "28/100 loss: 0.01824372448027134 acc: 0.9821826219558716\n",
      "29/100 loss: 0.01714998111128807 acc: 0.9832962155342102\n",
      "30/100 loss: 0.01615670882165432 acc: 0.9866369962692261\n",
      "31/100 loss: 0.015259611420333385 acc: 0.9877505302429199\n",
      "32/100 loss: 0.01445433497428894 acc: 0.9888641238212585\n",
      "33/100 loss: 0.013738085515797138 acc: 0.9910913109779358\n",
      "34/100 loss: 0.013095336966216564 acc: 0.9910913109779358\n",
      "35/100 loss: 0.012506493367254734 acc: 0.9910913109779358\n",
      "36/100 loss: 0.011954245157539845 acc: 0.9922049045562744\n",
      "37/100 loss: 0.011431064456701279 acc: 0.993318498134613\n",
      "38/100 loss: 0.010938710533082485 acc: 0.993318498134613\n",
      "39/100 loss: 0.010480182245373726 acc: 0.993318498134613\n",
      "40/100 loss: 0.010049087926745415 acc: 0.9944320917129517\n",
      "41/100 loss: 0.009642104618251324 acc: 0.9944320917129517\n",
      "42/100 loss: 0.009257622994482517 acc: 0.9944320917129517\n",
      "43/100 loss: 0.008900743909180164 acc: 0.9944320917129517\n",
      "44/100 loss: 0.008570443838834763 acc: 0.9944320917129517\n",
      "45/100 loss: 0.008263178169727325 acc: 0.9944320917129517\n",
      "46/100 loss: 0.007973922416567802 acc: 0.9944320917129517\n",
      "47/100 loss: 0.007698682602494955 acc: 0.9944320917129517\n",
      "48/100 loss: 0.007435685023665428 acc: 0.9944320917129517\n",
      "49/100 loss: 0.007184136193245649 acc: 0.9944320917129517\n",
      "50/100 loss: 0.0069432039745152 acc: 0.9944320917129517\n",
      "51/100 loss: 0.0067121367901563644 acc: 0.9944320917129517\n",
      "52/100 loss: 0.006490565370768309 acc: 0.9944320917129517\n",
      "53/100 loss: 0.006279593799263239 acc: 0.9955456852912903\n",
      "54/100 loss: 0.0060793571174144745 acc: 0.9955456852912903\n",
      "55/100 loss: 0.00588972819969058 acc: 0.9955456852912903\n",
      "56/100 loss: 0.005708717275410891 acc: 0.9955456852912903\n",
      "57/100 loss: 0.005535149481147528 acc: 0.9966592192649841\n",
      "58/100 loss: 0.005369377788156271 acc: 0.9977728128433228\n",
      "59/100 loss: 0.005210678558796644 acc: 0.9988864064216614\n",
      "60/100 loss: 0.005059204995632172 acc: 0.9988864064216614\n",
      "61/100 loss: 0.00491312937811017 acc: 0.9988864064216614\n",
      "62/100 loss: 0.004772367887198925 acc: 0.9988864064216614\n",
      "63/100 loss: 0.004635392222553492 acc: 0.9988864064216614\n",
      "64/100 loss: 0.00450245151296258 acc: 0.9988864064216614\n",
      "65/100 loss: 0.0043748002499341965 acc: 0.9988864064216614\n",
      "66/100 loss: 0.004252725746482611 acc: 0.9988864064216614\n",
      "67/100 loss: 0.004136525094509125 acc: 0.9988864064216614\n",
      "68/100 loss: 0.004026185721158981 acc: 0.9988864064216614\n",
      "69/100 loss: 0.003920707385987043 acc: 1.0\n",
      "70/100 loss: 0.003819308476522565 acc: 1.0\n",
      "71/100 loss: 0.003721768269315362 acc: 1.0\n",
      "72/100 loss: 0.0036278425250202417 acc: 1.0\n",
      "73/100 loss: 0.003537570359185338 acc: 1.0\n",
      "74/100 loss: 0.003450615331530571 acc: 1.0\n",
      "75/100 loss: 0.0033671248238533735 acc: 1.0\n",
      "76/100 loss: 0.003286853665485978 acc: 1.0\n",
      "77/100 loss: 0.00321019790135324 acc: 1.0\n",
      "78/100 loss: 0.0031364585738629103 acc: 1.0\n",
      "79/100 loss: 0.0030651227571070194 acc: 1.0\n",
      "80/100 loss: 0.0029964011628180742 acc: 1.0\n",
      "81/100 loss: 0.0029299326706677675 acc: 1.0\n",
      "82/100 loss: 0.0028657347429543734 acc: 1.0\n",
      "83/100 loss: 0.0028041063342243433 acc: 1.0\n",
      "84/100 loss: 0.0027444942388683558 acc: 1.0\n",
      "85/100 loss: 0.0026866979897022247 acc: 1.0\n",
      "86/100 loss: 0.0026307799853384495 acc: 1.0\n",
      "87/100 loss: 0.0025767062325030565 acc: 1.0\n",
      "88/100 loss: 0.0025243323761969805 acc: 1.0\n",
      "89/100 loss: 0.0024734523613005877 acc: 1.0\n",
      "90/100 loss: 0.0024241176433861256 acc: 1.0\n",
      "91/100 loss: 0.002376256976276636 acc: 1.0\n",
      "92/100 loss: 0.0023297991137951612 acc: 1.0\n",
      "93/100 loss: 0.0022847664076834917 acc: 1.0\n",
      "94/100 loss: 0.0022410957608371973 acc: 1.0\n",
      "95/100 loss: 0.002198813483119011 acc: 1.0\n",
      "96/100 loss: 0.002157776616513729 acc: 1.0\n",
      "97/100 loss: 0.0021179381292313337 acc: 1.0\n",
      "98/100 loss: 0.0020792584400624037 acc: 1.0\n",
      "99/100 loss: 0.002041697734966874 acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "model.train(train_x, y_train, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
